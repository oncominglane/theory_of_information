короче задача такая
у меня появился предмет Теория информации и по нему мне нужно раз в несколько лекций писать Эссе на тему - какой то питон-ноутбук файл, в котором я сам выбираю какую то тему связанную с лекцией и провожу на нее небольшое исследование 

сейчас я буду писать тебе конспект 1й лекции - какие основные моменты там обсуждались чтобы потом придумать тему для мини исследования

Лекция 1

Случайная величина = среднее огромного числа случ величин со средним m и дисперсией сигма то тогда это среднее имеет то же самое среднее и дисперсию дисперсию каждого / корень из N
(то есть дисперсия стремится к 0 при N огромных а значит )

В целом теория информации - применение термодинамических категорий к задачам обработки информации

Энтропия

Случ велич x может принимать M возможных значений (равновероятных) => для передачи информации о том что она приняла какое то значение нужно H=log2(M) бит
H=log2(M) - энтропия Хартли 

Если теперь исходы не равновероятны: каждому значению x -> p(x) и сумма p(x) = 1
Теперь говорим что -log2(p(x)) информации содержит исход вероячтность которого p(x)
А энтропия теперь это матожидание от среднего (-log2(p(x))) = -сумма (-p(x)*log(p(x))) - энтропия случайной величины
Такая энтропия совпадает с энтропией Хартли когда все иксы равновероятны, в произвольном случае она < H, то есть 0 <= h(x) <= H = log2(M) ( =0 когда одно p=1, остальные =0, случайная величина детерменированна)

доказательство этого неравенства - через неравенство логарифама
0 (детерменир случ велич) <= h(x) <= log2(M) (равномернораспредл случ велич)

неравенство Йенсена: для выпуклой f(x) верно E[f(x)] >= f(E[x])

двоичная энтропия - h(p) - энтропия бинарной случ величины, вероятность нуля p, вероятность единицы 1-p
h2(p) = -plog(p) (основание 2 - в битах, основание е - в натах)
симметрична относит прямой p=0,5 (и вы ней h = 1)

q-ичная энтропия - одно значение с вероятностью 1-p, остальные равновероятны с вероятностью p/(q-1)
ее энтропия h(p) = -(1-p)log2(1-p) - (q-1)*p/(q-1)log2(p/(q-1)) = - h2(p) + p*log2(q-1)
hq(p) = -p*logq(p) - (1-p)*logq(1-p) + p*logq(q-1)
график hq(p) !

Основа сжатия данных:
1) Кривая rate/log2(M) от Distortion - если допускаем хотя бы минимальные искажения,то скорость можем понизить с rate/log2(M)=1 до rate/log2(M)=H(x) - энтропии. 
2) более вероятные исходы - кодируем словами покороче и наоборот. ТОгда минимальная средняя длина такого слова будет равна H(x) - энтропии

число сочетаний из n по k = порядка = 2^(n*h(k/n)), h(k/n) - двоичная энтропия

информационная дивергенция D(P||Q) = сумма px * log(px/qx)


Лекция 2
график - при большом альфавите колокол првращается в прямую линию
определение информационной дивергенции между    распределениями 
